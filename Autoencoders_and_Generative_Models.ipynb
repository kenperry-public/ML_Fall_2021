{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYek5uDM0gmo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AutoEncoder (AE): High Level\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>The Deep Learning analog of Principal Components (PCA)</li>\n",
    "        <ul>\n",
    "            <li>Most of the lessons of AE apply equally to PCA</li>\n",
    "        </ul>\n",
    "        <li>Unsupervised: no labels (really semi-supervised)</li>\n",
    "        <li>Create \"synthetic features\" from the original set of features</li>\n",
    "        <li>May be able to use reduced set of synthetic features (dimensionality reduction)</li>\n",
    "        <li><b>Generative (vs Discriminative)</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "id": "biAjrkv4XJPi",
    "outputId": "89b91be6-062c-4145-de24-2cb79b81b94d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_vanilla.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An Autoencoder network has two parts\n",
    "- An Encoder, which takes input $\\x$ and \"encodes\" it into $\\z$\n",
    "- A Decoder, which takes the encoding $\\z$ and tries to reproduce $\\x$\n",
    "\n",
    "Each part has its own weights, which can be discovered through training, with examples\n",
    "- $\\langle \\X, \\y \\rangle = \\langle \\X, \\X \\rangle$\n",
    "\n",
    "That is: we are asking the output to be identical to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\z$ is an alternative latent representation of $\\x$.\n",
    "- Encoded by the Encoder\n",
    "- Inverted by the Decoder\n",
    "\n",
    "But\n",
    "when the dimension of $\\z$ is less than the dimension of $\\x$.\n",
    "- $\\z$ is a *bottle-neck*\n",
    "- the inversion by the Decoder will be imperfect\n",
    "\n",
    "$\\z$ becomes a *reduced-dimensionality* approximation of $\\x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is reminiscent of the dimensionality reduction of Principal Components Analysis (PCA).\n",
    "\n",
    "The *main difference* from PCA\n",
    "- PCA uses a *linear* transformation\n",
    "- NN can use *non-linear* transformations too\n",
    "    - PCA as a special case of AE\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYek5uDM0gmo",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Autoencoders: Uses\n",
    "\n",
    "## Dimension reduction\n",
    "\n",
    "After training\n",
    "- we can discard the Decoder\n",
    "- use the Encoder output (synthetic features) as reduced dimension inputs to a *new* task\n",
    "    - Encoder weights are **frozen**: non-learnable when training new task\n",
    "   - It may be easier to solve the new task given $\\z$ rather than $\\x$\n",
    "       - have already discovered \"structure\" of $\\x$\n",
    "   - *Transfer Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder: Encoder + New head</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_encoder_new_head.jpg\" width=1200></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In PCA, we eliminated original features that were \"less important\"\n",
    "- i.e., explained variation among only a small fraction of the training set\n",
    "    - recall how we re-denominated explained variance in terms of \"number of features\"\n",
    "    \n",
    "There is no direct similar concept of feature importance in AE\n",
    "- other than minimizing a Loss function, which *may* wind up focusing on \"important\" features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layer-wise pre-training with Autoencoders\n",
    "\n",
    "Autoencoders played a vital role in the development of Deep Learning:\n",
    "- They made it possible to train otherwise untrainable NN's.\n",
    "- Other innovations supplanted the need for AE's to assist training\n",
    "    - better initialization\n",
    "    - better activations functions\n",
    "    - normalization\n",
    " \n",
    "Although they are no longer needed for that purpose, it is interesting to see how (and why) they were used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a NN with $L$ layers that solves a Supervised Learning Problem\n",
    "- Training attempts to learn the weights of all layers simultaneously\n",
    "- *Layer wise pre-training* was an attempt\n",
    "    - to *initialize* the weights of each layer\n",
    "    - in succession\n",
    "    - so that the task of simultaneously solving for optimal weights had a better chance of succeeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea was to learn an initialization of $\\W_\\llp$, the weights of layer $l$.\n",
    "- After having learned the weights $\\W_{(l')}$ for all layers $l' \\lt l$.\n",
    "\n",
    "\n",
    "To initialize $\\W_\\llp$:\n",
    "- Train an AE that takes $\\x^\\ip$ as input\n",
    "- Using initialized weights $\\W_{(l')}$ for all layers $l' \\lt l$\n",
    "- Produces $\\tx^\\ip$ at layer $l$'s output $\\y_\\llp$\n",
    "\n",
    "So weight initializations were learned layer by layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the labels $\\y^\\ip$ *were not used* !\n",
    "- wouldn't be useful for the shallow NN\n",
    "\n",
    "It was thought\n",
    "- to be easier to learn the structure of the input $\\x$ independent of the labels\n",
    "- to be easier to learn $\\W_\\llp$ incrementally\n",
    "\n",
    "One the weights $\\W_\\llp$ were initialized via AE's\n",
    "- training of the Supervised Learning task had a better chance of succeeding\n",
    "- compared to any other initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders and Transfer Learning\n",
    "Today, autoencoders are useful for another purpose: Transfer Learning.\n",
    "\n",
    "If we can train an AE network to create features that are useful for reconstruction\n",
    "- it is possible\n",
    "that these features are useful for solving more complicated tasks.\n",
    "\n",
    "This was in essence what\n",
    "- Our dimension reduction example (replace the head) was doing\n",
    "- Layerwise Pre-training was attempting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So it is not uncommon to approach a complicated problem\n",
    "- by first constructing an autoencoder to\n",
    "come up with an alternate (and smaller) representation of the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that Autoencoders are *unsupervised*: they don't take labels.  \n",
    "\n",
    "So the encodings they produce\n",
    "stress syntactic similarity, rather than semantic similarity.\n",
    "\n",
    "Their use in Transfer Learning depends on the hope that inputs that are syntactically similar also\n",
    "have the same labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Denoising\n",
    "\n",
    "Very much like dimension reduction but with the assumption that\n",
    "- \"less important\" features are just random noise that is added to the true example\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder: Denoising</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_denoising.png\" width=1200></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Artificial Intelligence\n",
    "\n",
    "A less obvious use of AE (using the Decoder rather than the Encoder) is to *generate* examples.\n",
    "\n",
    "Most of the Machine Learning we have studied thus far is *discriminative*\n",
    "- $\\pr{\\hat{\\y}^\\ip | \\x^\\ip )}$\n",
    "    - e.g., classifier: discriminate among the possible classes $\\y^\\ip$, given example $\\x^\\ip$\n",
    "\n",
    "We can use the Decoder on *arbitrary* $\\z$ to *generate* a completely  new $\\x$:\n",
    "- $\\pr{ \\x^{(i')} | \\z^{(i')} }$ for some $i'$ not in training\n",
    "- *generate* a new example $i'$, in the domain of $\\x$, that was not encountered during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Generator</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_decoder.jpg\" width=800</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gHxF58P28q4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autoencoder (AE): Details\n",
    "\n",
    "The *task* that trains an Autoencoder\n",
    "- Given input $\\x^\\ip$\n",
    "- Output of Encoder: $\\z^\\ip = E(\\x^\\ip)$\n",
    "- Output of Decoder: $\\tx^\\ip = D(\\z^\\ip)$\n",
    "- \"Target\": $\\x^\\ip$\n",
    "\n",
    "Both the Encoder and Decoder are parameterized (learnable parameters)\n",
    "- Goal: find the parameters such that \n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    " \\tx^\\ip = D(E(\\x))  & \\approx & \\x \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$\\z^\\ip = E(\\x^\\ip)$ is the latent representation of $\\x^\\ip$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jm35Tb5vkgI-",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss function\n",
    "\n",
    "The obvious loss functions compare the original $\\x^\\ip$ and reconstructed $\\tilde\\x^\\ip$ feature by feature:\n",
    "\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "$$\n",
    "\\loss^\\ip = \\sum_{j=1}^{|\\x|} { (\\x^\\ip_j - \\tx^\\ip_j)^2 }\n",
    "$$\n",
    "\n",
    "### Binary Cross Entropy\n",
    "\n",
    "For the special case where *each* original feature is in the range $[0,1]$ (e.g., an image)\n",
    "\n",
    "$$\n",
    "\\loss^\\ip = \\sum_{j=1}^{|\\x|} {  \\left( \\x^\\ip_j    \\log(\\tx^\\ip_j) + ( 1 - \\x^\\ip_j ) \\log(1 - \\tx^\\ip_j) \\right) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFfhVRnM3C7q",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variational Autoencoder (VAE): Generative ML\n",
    "\n",
    "Observe that the Decoder part of the \"vanilla\" AE $D( \\z^\\ip )$\n",
    "- has been trained to produce \"realistic\" $\\tx^\\ip$ *only* for a $\\z^\\ip = E(\\x^\\ip)$\n",
    "    - i.e., \"realistic\": appears to come from the distribution of training $\\X$\n",
    "- there is no guarantee that $D( \\z^{(i')} )$ for some $i'$ not in training is realistic\n",
    "\n",
    "That is: the AE has not been trained to *extrapolate* beyond the training inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFfhVRnM3C7q",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A VAE is able generate outputs \n",
    "- that *could have* come from the training distribution from a latent representation $\\z^{(i')}$ \n",
    "- but that *did not* come from $\\X$.\n",
    "\n",
    "Our goal is constructing a **Decoder** that can extrapolate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Variational Autoencoder (VAE)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFfhVRnM3C7q",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Decoder will take a *latent vector* $\\z$ and produce $D(\\z)$, just as in a vanilla AE.\n",
    "\n",
    "\n",
    "The difference is that $\\z$ will be sampled from a *distribution* rather than being a \n",
    "unique mapping of a training example.\n",
    "\n",
    "This will be done by modifying the Encoder\n",
    "- It will *indirectly* create $\\z^\\ip$\n",
    "- It will compute *variables* $\\mu^\\ip$ and $\\sigma^\\ip$\n",
    "    - $\\z^\\ip$ will be *sampled* from a distribution with mean $\\mu^\\ip$ and standard deviations $\\sigma^\\ip$\n",
    "    \n",
    "As long as $\\z$ is sampled from this distribution, the decoder will produce a \"realistic\" output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "$\\mu$ and $\\sigma$ are \n",
    "- vectors\n",
    "- computed values (and hence, functions of $\\x$) and **not** parameters\n",
    "- so training learns a *function* from $\\x^\\ip$ to $\\mu^\\ip$ and $\\sigma^\\ip$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFfhVRnM3C7q",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To train a VAE:\n",
    "- pass input $\\x^\\ip$ through the Encoder, producing $\\mu^\\ip, \\sigma^\\ip$\n",
    "    - use $\\mu^\\ip, \\sigma^\\ip$ to sample a latent representation $\\z^\\ip$ from the distribution\n",
    "- pass the sampled $\\z^\\ip$ through the decoder, producing $D(\\z^\\ip)$\n",
    "- measure the reconstruction error $\\x^\\ip - D(\\z^\\ip)$, just as in a vanilla AE\n",
    "- back propagate the error, updating all weights and $\\mu, \\sigma$\n",
    "\n",
    "Essentially, each input $\\x^\\ip$ has *many* latent representations (with different probabilities):\n",
    "any sample from the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbWCKt7Or89C",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Training**\n",
    "\n",
    "Encoder produces\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "E(\\x) & = &  q_\\phi(\\z|\\x) & \\approx & p_\\theta(\\z|\\x) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We sample from\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\hat{\\z} & \\sim & q_\\phi(\\z|\\x) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Decoder produces\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "D(\\hat{\\z})  & = & p_\\theta (\\x|\\z)\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Each time (epoch) that we encounter the same training example, we select another random element from the distribution.\n",
    "\n",
    "So the VAE learns to represent the same example from multiple latents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbWCKt7Or89C",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Generative**\n",
    "- sample $\\hat{\\z} \\sim \\hat{p}(\\z)$\n",
    "- use decoder to produce output $p_\\theta (\\x|\\z)$\n",
    " \n",
    "This means we can feed in a $\\z$ \n",
    "- that doesn't correspond to any training example\n",
    "- and perhaps get an output that *resembles* something from the training set, rather than noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Which came first: the VAE architecture or the loss function ?\n",
    "\n",
    "So far, we haven't told you the Loss function that is optimized during training.\n",
    "\n",
    "It's a bit complicated so we'll save it for the end (for those who are interested).\n",
    "\n",
    "The really interesting thing:\n",
    "- The Loss function drove the architecture of the VAE, not vice-versa !\n",
    "- As we've said many times in this course: \n",
    "    - Deep Learning is about creating Loss functions that reflect our objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFfhVRnM3C7q",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditional VAE\n",
    "\n",
    "Once a VAE is trained, $D(\\z)$ should produce a realistic output, for any $\\z$ from the distribution.\n",
    "\n",
    "However, if the distribution of $\\X$ includes examples from many classes \n",
    "- Assuming we have labels as auxilliary information (not used in training)\n",
    "    - e.g., the 10 digits\n",
    "- The VAE can't control *which class* the output will come from\n",
    "\n",
    "A *Conditional VAE* allow our generator (Decoder) to control the class $c$ of the output $\\tx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Conditional VAE (CVAE)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_CVAE.jpg\"\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kaLKMwF5bOR",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The class label $c$\n",
    "- is given as part of *training*\n",
    "    - So the Encoder produces a distribution that is conditioned on *both* $\\x$ and $c$.\n",
    "- is an *additional parameter* of the Decoder\n",
    "    - So the output class can be controlled\n",
    "$$\n",
    "\\tx^\\ip = D(\\z^\\ip, c)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kaLKMwF5bOR",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So now we \n",
    "- create a latent $\\z$\n",
    "- append a class label $c$\n",
    "- and presumably have the decoder produce an output from the desired class.\n",
    "\n",
    "- The encoding distribution is now conditional on class label $c$: $q_\\phi(z|x,c)$ \n",
    "- So is the decoding distribution $p_\\theta(x|z,c)$ \n",
    "\n",
    "Again, by restricting the functional form of the prior distribution $\\hat{p}$ we can simplify the math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detour: Autoencoder notebook on Colab\n",
    "\n",
    "Let's examine some Keras code that implements several types of Autoencoders\n",
    "- Vanilla\n",
    "- Denoising\n",
    "- VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will write our AE's using the Keras *Functional* API rather than the *Sequential* model\n",
    "- We *could* write the complete AE using the Sequential API\n",
    "- **But**\n",
    "    - we want to extract the Encoder and Decoder parts as *separate models*\n",
    "    - we can do this with the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will now switch to a notebook running on Google Colab\n",
    " [Autoencoder example from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2019/blob/master/Autoencoder_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VAE: Probabilistic formulation\n",
    "\n",
    "**Note**: Advanced material\n",
    "\n",
    "Let's pretend: we don't already know that we will represent $\\mathbf{z}$ as a function of $\\mathbf{\\mu}_\\theta(\\x)$ and $\\mathbf{\\sigma}_\\theta(\\x)$\n",
    "- this derivation will show **why** we made that choice\n",
    "\n",
    "The mathematical derivation of a VAE is quite detailed\n",
    "- it is interesting but not absolutely necessary to understand\n",
    "- this is where we define the Loss function\n",
    "\n",
    "The interested reader is refered to a highly recommended [VAE tutorial](https://arxiv.org/pdf/1606.05908.pdf).\n",
    "\n",
    "We will try to give the essence in the following slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>The VAE has a very interesting <b>two part</b> Loss Function</li>\n",
    "        <ul>\n",
    "            <li>Reconstruction Loss, as in the Vanilla AE</li>\n",
    "            <li>Divergence Loss\n",
    "        </ul>\n",
    "        <li>The Reconstruction Loss is not sufficient</li>\n",
    "        <ul>\n",
    "            <li>Issues of intractability arise</li>\n",
    "            <li>The Divergence Loss skirts intractability</li>\n",
    "            <ul>\n",
    "                <li>By constraining the Encoder to produce a tractable distribution</li>\n",
    "            </ul>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From the description of the VAE, observe that we are now dealing with distributions rather\n",
    "than deterministic values for\n",
    "- the encoding (latent representation) $\\z$\n",
    "- the output\n",
    "\n",
    "So we will need to describe these distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We posit a  joint probabiity distribution $p(\\x, \\z)$ of examples and latents.\n",
    "\n",
    "These are *empirical* distributions:\n",
    "- they are defined by the data\n",
    "- no closed form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will approximate $p$, as usual, with a NN that we will parameterize with $\\theta$.\n",
    "\n",
    "That is: we will train a model to learn $\\theta$.\n",
    "\n",
    "So henceforth we write $p_\\theta$ to denote the dependence of $p$ on parameters $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We motivate these distributions as they relate to the VAE:\n",
    "- the encoder produces $p_\\theta(\\z|\\x)$\n",
    "- the decoder produces $p_\\theta(\\x|\\z)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>VAE derivation: 1</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE_derivation_A.png\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that there are *many* reconstructions $\\tilde\\x^\\ip$ of $\\x^\\ip$\n",
    "- depending on the sampled $\\tilde\\z^\\ip$ drawn from $p_\\theta(\\z | \\x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The loss function: first attempt\n",
    "\n",
    "Let's try to create an optimization objective function against which we choose our model weights.\n",
    "\n",
    "We will use Maximum Likelihood as the objective\n",
    "- Given the weights: how likely is the model to produce the training distribution ?\n",
    "\n",
    "Since our practice is to minimize Loss (rather than maximize an objective function)\n",
    "we write our loss as (negative of log) likelihood\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\loss & = & - \\log( p_\\theta(\\x) )\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Minimizing $\\loss$ is equivalent to maximizing likelihood.\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intractability\n",
    "\n",
    "It turns out that things are not so simple: \n",
    "- Some of the distributions we need to deal with \n",
    "may not be *tractable*\n",
    "\n",
    "- They have no closed form, just empirical distributions\n",
    "- Higher dimensional distributions may  pose computational issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem is that\n",
    "$p_\\theta( \\z | \\x)$ is assumed to be intractable\n",
    "\n",
    "That is: for a given $\\x^\\ip$, we don't know which samples from $\\z$ can reconstruct $\\x^\\ip$.\n",
    "    \n",
    "$p_\\theta(\\z|\\x) = \\frac{p_\\theta(\\x|\\z) p(\\z)}{p_\\theta(\\x)}$ (by Bayes rule)\n",
    "- this represents the distribution for the encoder\n",
    "- $\\z$ is from an unknown distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The solution is *approximate* the intractable $p_\\theta(\\z|\\x)$ with a tractable $q_\\phi(\\z|\\x)$\n",
    "- where $q_\\phi(\\z|\\x)$ is a function of learnable parameters $\\phi$.\n",
    "\n",
    "We use KL divergence as a measure of the difference between two distributions.\n",
    "\n",
    "So we want to minimize\n",
    "\n",
    "$$\n",
    "\\KL( q_\\phi(\\z|\\x) \\; ||\\; p_\\theta(\\z | \\x))\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>VAE derivation: 2</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE_derivation_B.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We add the KL divergence to our loss function\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss  & = & - \\log(p_\\theta(\\x)) + \\KL( q_\\phi(\\z|\\x) \\; ||\\; p_\\theta(\\z | \\x)) \\\\\n",
    "& = & \\loss_R + \\loss_D\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "which now has two objectives\n",
    "- Reconstruction loss $\\loss_R$: maximize the likelihood (by minimizing the negative likelihood)\n",
    "- Divergence constraint $\\loss_D$: $q_\\phi(\\z|\\x)$ must be close to $p_\\theta(\\z | \\x))$\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\loss_R & = & - \\log( p_\\theta(\\x ) ) \\\\\n",
    "\\loss_D & = & \\KL \\left(  q_\\phi(\\z|\\x) \\; || \\; p_\\theta(\\z | \\x) \\right) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will show (in the next section: lots of algebra !) that\n",
    "$$\n",
    "\\loss = - \\E_{z \\sim q_\\phi(\\z|\\x)}\\left( \\log( p_\\theta(\\x | \\z) ) \\right) + \\KL(q_\\phi(\\z|\\x)  \\; ||\\;  p_\\theta(\\z) )\n",
    "$$\n",
    "\n",
    "This is *almost* identical to $\\loss$ except\n",
    "- Re-write \n",
    "$$\\log(p_\\theta(\\x)) = \n",
    "\\E_{z \\sim q_\\phi(\\z|\\x)}\\left( \\log( p_\\theta(\\x | \\z) ) \\right)\n",
    "$$\n",
    "- the KL term becomes\n",
    "$$\n",
    " \\KL \\left(  q_\\phi(\\z|\\x) \\; || \\; p_\\theta(\\z) \\right)\n",
    "$$\n",
    "rather than the original\n",
    "$$\n",
    "\\KL \\left(  q_\\phi(\\z|\\x) \\; || \\; p_\\theta(\\z | \\x) \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is, our  new loss $\\loss$ function has two components\n",
    "- $\\loss_R$\n",
    "    - the reconstruction loss, as before, but using the encoder $q_\\phi(\\z|\\x)$ instead of $p_\\theta(\\z|\\x)$\n",
    "\n",
    "- $\\loss_D$\n",
    "    - the \"KL divergence\" loss which constrains the approximate $q_\\phi(\\z|\\x)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Advanced: Obtain $\\loss$ by rewriting $\\KL( q_\\phi(\\z|\\x) \\; ||\\; p_\\theta(\\z | \\x)$\n",
    "\n",
    "Let's derive a simpler expression for $\\loss$ by manipulating $\\KL( q_\\phi(\\z|\\x) \\; ||\\; p_\\theta(\\z | \\x))$:\n",
    "\n",
    "$\n",
    "\\begin{array}[llll]\\\\\n",
    "\\KL( q_\\phi(\\z|\\x) \\; ||\\; p_\\theta(\\z | \\x)) &  = & \\sum_z{ q_\\phi(\\z|\\x )(\\log(q_\\phi(\\z|\\x) - \\log(p_\\theta(\\z | \\x)) } & \\text{def. of KL} \\\\\n",
    "&  = & \\E_{z \\sim q_\\phi(\\z|\\x) } \\left( \\log(q_\\phi(\\z|\\x) - \\log(p_\\theta(\\z | \\x)) \\right) & \\text{def. of }\\E \\\\\n",
    "&  = & \\E_{z \\sim q_\\phi(\\z|\\x) } ( \\; \\log(q_\\phi(\\z|\\x)) \\\\ & & -\\left( \\; \\log( p_\\theta(\\x | \\z)) + \\log(p_\\theta(\\z)) - \\log(p_\\theta(\\x) \\right)    \\,   )  \\;\\;)&  \\text{Bayes theorem on } \\\\\n",
    " & & & \\log(p_\\theta(\\z | \\x)) \\\\\n",
    "\\KL( q_\\phi(\\z|\\x) \\; ||\\; p_\\theta(\\z | \\x)) \\\\ - \\log(p_\\theta(\\x)) & = & \\E_{z \\sim q_\\phi(\\z|\\x) } \\left( \\; \\log(q_\\phi(\\z|\\x))  - \\left( \\log( p_\\theta(\\x | \\z) ) + \\log( p_\\theta(\\z) ) \\right) \\;\\right) & \\text{ move } \\log(p_\\theta(\\x)) \\text{ to LHS} \\\\\n",
    " & = & \\E_{z \\sim q_\\phi(\\z|\\x) } \\left( \\; - \\log( p_\\theta(\\x | \\z) ) + ( \\; \\log(q_\\phi(\\z|\\x))  - \\log( p_\\theta(\\z) ) \\; )     \\; \\right) & \\text{re-arrange terms} \\\\\n",
    " & = & - \\E_{z \\sim q_\\phi(\\z|\\x) } \\left( \\log( p_\\theta(\\x | \\z) ) \\right) + \\KL(q_\\phi(\\z|\\x) \\; ||\\;  p_\\theta(\\z) ) & \\text{def. of KL} \\\\\n",
    " \\loss & = & - \\E_{z \\sim q_\\phi(\\z|\\x) } \\left( \\log( p_\\theta(\\x | \\z) ) \\right) + \\KL(q_\\phi(\\z|\\x) \\; ||\\;  p_\\theta(\\z) ) & \\text{since LHS} = \\loss \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The LHS cannot be optimized via SGD (recall the tractability issue with  $p_\\theta(\\z|\\x)$).\n",
    "\n",
    "**But the RHS can be made tractable** giving a tractable choice of $p_\\theta(\\z)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choosing $p_\\theta(\\z)$\n",
    "\n",
    "So what distribution should we use for the prior $p_\\theta(\\z)$ ?\n",
    "- It should be differentiable, since we use Gradient Descent for optimization\n",
    "- It would be advantageous if it had a tractable closed form (such as a normal)\n",
    "- If we choose $p_\\theta(\\z)$ as normal, we can require $q_\\phi( \\z | \\x )$ to be normal too\n",
    "    - The KL divergence between two normals is an easy to compute function of their means and standard deviations.\n",
    "    - See [VAE tutorial](https://arxiv.org/pdf/1606.05908.pdf) Section 2.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So it may be fair to say that the idea for architecture of the VAE was obtained from the Loss function,\n",
    "rather than vice-versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnFvZdJ1sV_e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Re-parameterization trick\n",
    "\n",
    "There is still one more problem for training:\n",
    "- sampling $\\z  \\sim  q_\\phi(\\z|\\x)$\n",
    "which appears in the reconstruction loss $\\loss_R$ term of the loss $\\loss$:\n",
    "$$\n",
    "\\loss_R = \\E_{z \\sim q_\\phi(\\z|\\x) } \\left( \\log( p_\\theta(\\x | \\z) ) \\right)\n",
    "$$\n",
    "\n",
    "This is not a problem in the forward pass.\n",
    "\n",
    "But optimization using Gradient Descent requires the ability to differentiate the loss with respect to trainable paramters.\n",
    "- And we don't know how to back propagate through a stochastic node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnFvZdJ1sV_e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In particluar\n",
    "$$\n",
    "\\frac{\\loss_R}{\\partial \\phi}\n",
    "$$\n",
    "\n",
    "involves a random sample $z \\sim q_\\phi(\\z|\\x)$ from a function of $\\phi$.\n",
    "\n",
    "How do we take the derivative of a node involving a random choice ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnFvZdJ1sV_e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"reparameterization trick\"\n",
    "- redefines $\\z$\n",
    "- as the transformation of a distribution $p(\\z)$\n",
    "    - (Looking ahead: which will wind up as unit normal distribution $N(0,1)$)\n",
    "- by scaling it by a standard deviation and adding a mean\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\mathbf{z}  & = & \\mathbf{\\mu}_\\theta(\\x) + \\mathbf{\\sigma}_\\theta(\\x) \\mathbf{\\epsilon} \\\\\n",
    "\\mathbf{\\epsilon} & \\sim & p(\\mathbf{z}) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Reparameterization trick</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Reparameterization_trick.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnFvZdJ1sV_e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key is that the mean $\\mathbf{\\mu}_\\theta(\\x)$ and standard deviation $\\mathbf{\\sigma}_\\theta(\\x)$\n",
    "- are *functions* of input $\\x^\\ip$\n",
    "    - so the encoding depends on the input\n",
    "- and a function of trainable parameters $\\theta$ and not $\\phi$\n",
    "    - so neither affects $\\frac{\\loss_R}{\\partial \\phi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnFvZdJ1sV_e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We still can't take derivatives of $L_R$ with respect to $\\epsilon$, but we don't need to !\n",
    "- it's not a trainable parameter\n",
    "\n",
    "We only need to take derivates of $L_R$ with respect to $\\phi, \\theta, \\mu, \\sigma$, which we can do.\n",
    "\n",
    "In evaluating derivatives, the $\\epsilon$ that appears in the result (e.g., derivative wrt $\\sigma$) can be treated as a constant.\n",
    "\n",
    "- For a particular example, we can remember the  drawn $\\epsilon$ in the forward pass and use it in the backward pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This gets us to the (near) final picture of the VAE:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Variational Autoencoder (VAE)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Reparameterization trick</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Reparameterization_trick.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ELBo lower bound\n",
    "To summarize\n",
    "- we still have an intractable term (appears as another $\\KL$ divergence after re-writing loss\n",
    "    - this term appears as an additive term\n",
    "    - by definition of $\\KL$, it is positive\n",
    "- so we can't evaluate the full loss function\n",
    "    - but, ignoring the intractable positive part, the remainder is a *lower bound* on the loss\n",
    "        - so we optimize the lower bound\n",
    "        - called the *ELBO* term (LB is lower bound)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVIedxZelBZN",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss function: discussion\n",
    "\n",
    "Let's examine the role of $\\loss_R$ and $\\loss_D$ in the loss function $\\loss$.\n",
    "\n",
    "- What would happen if we dropped $\\loss_D$ ?\n",
    "    - We would wind up with a deterministic $\\z$ and collapse to a vanilla VAE\n",
    "    \n",
    "- What would happen if we dropped $\\loss_R$ ?\n",
    "    - the encoding approximation $q_\\phi(\\z|\\x)$ would be close to the empirical $p_\\theta(\\z | \\x)$ *in distribution*\n",
    "    - but two variables with the same distribution are not necessarily the same ?\n",
    "        - e.g., get a distribution $p$ by flipping a coin\n",
    "            - let distribution $q$ be a relabelling of $p$ by changing Heads to Tails and vice-versa\n",
    "            - $p$ and $q$ are equal in distribution but clearly different !\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Autoencoders_and_Generative_Models.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

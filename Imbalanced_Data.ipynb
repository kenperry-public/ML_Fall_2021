{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\o}{\\mathbf{o}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imbalanced datasets\n",
    "\n",
    "What happens when our *training examples* are imbalanced\n",
    "- some examples over-represented\n",
    "- other examples under-represented\n",
    "\n",
    "We already briefly covered this in [Loss Analysis](Training_Loss.ipynb#Conditional-loss)\n",
    "- Our motivation there was focusing on examples where errors occur\n",
    "- Here our motivation is when the examples naturally partition into *imbalanced* subsets\n",
    "\n",
    "\n",
    "We revisit this in the case of a Binary Classification task, where one class dominates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Training loss is usually given unconditionally:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training Example</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_error_analysis.png\"</td>\n",
    "    </tr>\n",
    "</table>\n",
    "â€¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But we can also partition the examples and examine the loss in each partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Loss analysis: conditional loss</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_error_analysis_1.png\"</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we partition the training examples into those whose class is Positive and those whose class is Negative:\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\langle \\X, \\y \\rangle & = & [ \\x^\\ip, \\y^\\ip | 1 \\le i \\le m ] \\\\\n",
    " & = & [ \\x^\\ip, \\text{Positive} | 1 \\le i \\le m' ] \\; \\cup \\; [ \\x^\\ip, \\text{Negative} | 1 \\le i \\le m'' ] \\\\ \n",
    " & = & \\langle \\X_{(\\text{Positive})}, \\y_{(\\text{Positive})} \\rangle \\; \\cup \\; \\langle \\X_{(\\text{Negative})}, \\y_{(\\text{Negative})} \\rangle\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can partition the training loss\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss_\\Theta  & = & { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta \\\\\n",
    "& = & \\frac{m'}{m} { 1\\over{m'} } \\sum_{i' \\in \\X_{(\\text{Positive})}} \\loss^\\ip_\\Theta \\; + \\; \\frac{m''}{m} { 1\\over{m''} } \\sum_{i'' \\in \\X_{(\\text{Negative})}} \\loss^\\ip_\\Theta\n",
    "\\end{array}\n",
    "$$\n",
    "That is, the Average loss is the weighted (with weights $\\frac{m'}{m}, \\frac{m''}{m}$) conditional losses\n",
    "- ${ 1\\over{m'} } \\sum_{i' \\in \\X_{(\\text{Positive})}} \\loss^\\ip_\\Theta$\n",
    "-  ${ 1\\over{m''} } \\sum_{i'' \\in \\X_{(\\text{Negative})}} \\loss^\\ip_\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we've observed before\n",
    "- As long as the majority class dominates in count (e.g., $m' \\gg m''$)\n",
    "- It is possible for Average Loss to be low\n",
    "- Even if Conditional Loss for the minority class is high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This means that training is less likely to generalize well out of sample to the minority examples.\n",
    "\n",
    "When the set of training examples $\\langle \\X, \\y \\rangle$ is such that\n",
    "- $\\y$ comes from set of categories $C$\n",
    "- Where the distribution of $c \\in C$ is *not* uniform\n",
    "\n",
    "the dataset is called *imbalanced*.\n",
    "\n",
    "This means that training is may be biased to not do as well on examples from under-represented classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the Titanic survival:\n",
    "- only 38% of the passengers survived, so the dataset is highly imbalanced\n",
    "- a naive model that *always* predicted \"Not survived\" will\n",
    "    - have 62% accuracy\n",
    "        - be correct 100% of the time for 62% of the sample (those that didn't survive)\n",
    "        - be incorrect 100% of the time for 38% of the sample (those that did survive)\n",
    "\n",
    "The question is whether your use case requires high accuracy in *all* classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approaches to imbalanced training data\n",
    "\n",
    "There are a number of approaches to avoid a potential bias caused by imbalanced data.\n",
    "\n",
    "[The `imbalanced-learn` website](https://imbalanced-learn.org/stable/user_guide.html) documents approaches to this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Loss\n",
    "- Use conditional metrics rather than unconditional metrics\n",
    "    - Metric less influenced by size\n",
    "    - Combination of Precision and Recall\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choose a model that is not sensitive to imbalance\n",
    "- Decision Trees\n",
    "    - branching structure can handle imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss sensitive training\n",
    "\n",
    "Modify the Loss function to weight conditional probabilities\n",
    "\n",
    "Rather than\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss_\\Theta  & = & { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta \\\\\n",
    "& = & \\frac{m'}{m} { 1\\over{m'} } \\sum_{i' \\in \\X_{(\\text{Positive})}} \\loss^\\ip_\\Theta \\; + \\;  \\frac{m''}{m} { 1\\over{m''} } \\sum_{i'' \\in \\X_{(\\text{Negative})}} \\loss^\\ip_\\Theta\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "adjust weights\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss_\\Theta  & = & \n",
    "C_{\\text{Positive}} * \\sum_{i' \\in \\X_{(\\text{Positive})}} \\loss^\\ip_\\Theta \\; + \\; C_{\\text{Negative}} * \\sum_{i'' \\in \\X_{(\\text{Negative})}} \\loss^\\ip_\\Theta\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Equally weighted across classes: $C_\\text{Positive} = C_\\text{Negative}$\n",
    "- Relative importance\n",
    "    - An error in one class may be more important than an error in the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `sklearn` inverse frequency weights\n",
    "    - user-defined weights (`sklearn` optional `class_weights` argument to some classification models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resampling\n",
    "\n",
    "Re-sampling is a process of constructing a new set of training examples by\n",
    "drawing samples from the original.\n",
    "\n",
    "The sampling doesn't have to be uniform and may be used such that the resampled examples are more balanced\n",
    "- We can oversample (draw with higher probability) the Minority class\n",
    "- We can undersample (draw with lower probability) the Majority class\n",
    "\n",
    "In the limit, weighted sampling is almost equivalent to using the original examples, but\n",
    "weighting the loss term for each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>Imbalanced data: Oversample the minority</strong></center>\n",
    "    </tr>\n",
    "<img src=images/Oversample.png>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>Imbalanced data: Undersample the majority</strong></center>\n",
    "    </tr>\n",
    "<img src=images/Undersample.png>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Caveat:**  No \"cheating\" in cross validation, again\n",
    "\n",
    "Does it make a difference if\n",
    "- We resample before cross validation ?\n",
    "- We resample *during* cross validation ?\n",
    "\n",
    "Yes !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- By resampling before cross validation\n",
    "    - the fold that is out-of-sample comes from the more-balanced resampled distribution\n",
    "- By resampling during cross validation\n",
    "    - Create the folds, including the out of sample fold\n",
    "    - Resample from the remaining folds to get a more balanced resampled distribution for this iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Is there a **simple** way to resample without cheating ?\n",
    "\n",
    "We can resample during cross-validation by making the resampling step the first element of a Pipeline\n",
    "- See the lecture on Cross Validation without cheating\n",
    "- PROBLEM: we would also need to modify the target, which `sklearn` does not yet support\n",
    "    - See [this proposal](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep001/proposal.html) to enhance Transformers to support target transformation as well\n",
    "- ISSUE: if we apply this Pipeline consistently (to both training and test examples)\n",
    "    - What does this do to the distribution of test examples ?\n",
    "    - It would make the training and test distributions similar IF we were given multiple test examples at once\n",
    "   - Real world behavior: test examples are presented \"on-line\": one at a time, not as a group\n",
    "        - We can't rebalance a single example\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The question is: \"What are we trying to achieving by resampling ?\"\n",
    "\n",
    "If our goal is to get the Loss Function to place more emphasis (greater weight) on examples from the under represented class:\n",
    "- use `sklearn`'s `class_weights` optional argument\n",
    "- Similar effect (on the Loss function) to rebalancing, but without the side effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Synthetic examples\n",
    "\n",
    "Oversampling merely duplicates existing examples from the Minority class.\n",
    "\n",
    "There are techniques to generate *synthetic* examples\n",
    "- SMOTE\n",
    "- ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A rough description of creating a synthetic example\n",
    "- Choose an example; find \"close\" neighboring examples of the same class (Minority)\n",
    "    - Using a metric of distance between examples\n",
    "- Create an example that blends/interpolates features from the neighbors into a new example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imbalanced example\n",
    "Here's an unbalanced dataset with 2 classes, and the associated predictions of some model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_true = np.array([0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([0, 1, 0, 0, 0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And the regular and class balanced accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acccuracy=0.667\n",
      "Class balanced Acccuracy=0.625\n"
     ]
    }
   ],
   "source": [
    "print(\"Acccuracy={a:3.3f}\".format(a=accuracy_score(y_true,  y_pred)))\n",
    "print(\"Class balanced Acccuracy={a:3.3f}\".format(a=balanced_accuracy_score(y_true,  y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's the math behind the two accuracy computations\n",
    "- \"Regular accuracy\": per class conditional accuracy, weight by class fraction as percent of  total\n",
    "- \"Balanced accuracy\": simple average of per class conditional accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy conditional on class=0 (66.67% of examples) = 0.750\n",
      "Accuracy conditional on class=1 (33.33% of examples) = 0.500\n",
      "\n",
      "\n",
      "Computed Accuracy=0.667 ( 66.67% * 0.750 + 33.33% * 0.500 )\n",
      "Computed Balanced Accuracy=0.625 ( average( 0.75, 0.5) )\n"
     ]
    }
   ],
   "source": [
    "# Enumerate the classes\n",
    "classes = [0,1]\n",
    "\n",
    "accs, weights = [], []\n",
    "\n",
    "# Compute per class accuracy and fraction\n",
    "for c in classes:\n",
    "    # Filter examples and predictions, conditional on class == c\n",
    "    cond = y_true == c\n",
    "    y_true_cond, y_pred_cond = y_true[cond],  y_pred[cond]\n",
    "    \n",
    "    # Compute fraction of total examples in class c\n",
    "    fraction = y_true_cond.shape[0]/y_true.shape[0]\n",
    "    \n",
    "    # Compute accuracy on this single class\n",
    "    acc_cond = accuracy_score(y_true_cond, y_pred_cond)\n",
    "    \n",
    "    print(\"Accuracy conditional on class={c:d} ({p:.2%} of examples) = {a:3.3f}\".format(c=c, \n",
    "                                                                                        p=fraction,\n",
    "                                                                                        a=acc_cond\n",
    "                                                                                     )\n",
    "         )\n",
    "    \n",
    "    accs.append(acc_cond)\n",
    "    weights.append(fraction)\n",
    "\n",
    "# Manual computation of accuracy, to show the math\n",
    "acc = np.dot( np.array(accs), np.array(weights) )\n",
    "acc_bal = np.average( np.array(accs) )\n",
    "\n",
    "eqn_elts = [ \"{p:.2%} * {a:3.3f}\".format(p=fraction, a=acc_cond) for fraction, acc_cond in zip(weights, accs) ]\n",
    "eqn_bal = \" + \".join( eqn_elts )\n",
    "\n",
    "eqn = \"average( {elts:s})\".format(elts=\", \".join([ str(a) for a in accs ]))\n",
    "print(\"\\n\")\n",
    "print(\"Computed Accuracy={a:3.3f} ( {e:s} )\".format(a=acc, e=eqn_bal))\n",
    "print(\"Computed Balanced Accuracy={a:3.3f} ( {e:s} )\".format(a=acc_bal, e=eqn) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "368px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import neural_net_helper\n",
    "%aimport neural_net_helper\n",
    "\n",
    "nnh = neural_net_helper.NN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vanishing/exploding gradients\n",
    "\n",
    "Now that we have a better view of how backward propagation of gradients work, we are equipped\n",
    "to understand the difficulties of training the weights.\n",
    "\n",
    "Until the problems were understood, and solutions found, the evolution of Deep Learning\n",
    "was extremely slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's summarize back propagation up until this point\n",
    "- We compute the loss gradient $\\loss'_\\llp = \\frac{\\partial \\loss}{\\partial \\y_\\llp}$ of each layer $\\ll$ in descending order\n",
    "\n",
    "- The backward step  to compute the loss gradient of the preceding layer is:  \n",
    "    - $\\loss'_{(\\ll-1)} =  \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}$\n",
    "\n",
    "When we derived back propagation, we didn't look \"inside\" of the \"local gradient \" $\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}$\n",
    "\n",
    "We will do so now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look more deeply into the  term $\\frac{\\partial \\y_\\llp}{\\partial \\y_{(i-1)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}[lllll] \\\\\n",
    "\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} & = & \\frac{\\partial a_\\llp ( f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp))}{\\partial \\y_{(\\ll-1)}} & (\\text{def. of } \\y_\\llp) \\\\\n",
    "                                      & = & \\frac{\\partial a_\\llp ( f_\\llp(\\y_{(\\ll-1)}, W_\\llp) )}{\\partial f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp)} \\frac{\\partial f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp)}{\\partial \\y_{(\\ll-1)}} &  (\\text{chain rule}) \\\\\n",
    "                                      & = a'_\\llp f'_\\llp\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where we define\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "a'_\\llp & = & \\frac{\\partial a_\\llp ( f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp) )}{\\partial f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp)}  & \\text{derivative of } a_\\llp(\\ldots) \\text{ wrt } f_\\llp(\\ldots)\\\\\n",
    "f'_\\llp & = & \\frac{\\partial f_\\llp(\\y_{(\\ll-1)}, W_\\llp)}{\\partial \\y_{(\\ll-1)}} & \\text{derivative of } f_\\llp(\\ldots) \\text{ wrt } \\y_{(\\ll-1)}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$a'_\\llp$ is the derivative of activation function $a_\\llp$.\n",
    "\n",
    "We won't explicitly write it out other than to observe $a'_\\llp \\in [0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Substituting the value of the loss gradient into the backward update rule:\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll]\\\\\n",
    "\\loss'_{(\\ll-1)} & = &  \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} \\\\\n",
    "         & = &  \\loss'_\\llp a'_\\llp f'_\\llp\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Hopefully, you can see that if iterate through single backward steps, we can derive\n",
    "an expression for the loss gradient at layer $\\ll$ in terms of the loss gradient\n",
    "of the final layer $K$:\n",
    "\n",
    "Since\n",
    "$$\\loss'_\\llp  =   \\loss'_{(\\ll+1)} \\frac{\\partial \\y_{(\\ll+1)}}{\\partial \\y_\\llp}$$\n",
    "\n",
    "we get\n",
    "$$\\loss'_\\llp  =   \\loss'_{(L+1)} \\prod_{l'=\\ll+1}^L  a'_{(l')} f'_{(l')}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The issue is that, since \n",
    "$$\n",
    "0 \\le a'_\\llp \\le \\max{z} a'_\\llp(z)\n",
    "$$\n",
    "\n",
    "the product \n",
    "$$\\prod_{l'=i+1}^K {a'_{(l')}}\n",
    "$$\n",
    "can be increasingly small as the number of layers $K$ grows, if $\\max{z} a'_\\llp(z) < 1$.\n",
    "\n",
    "Note, for $a_\\llp = \\sigma$ (the sigmoid function), $\\max{z} a'_\\llp(z) = 0.25$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, unless offset by the $f'_\\llp$ terms, $\\loss'_\\llp$ will quickly diminish to $0$ as $K$ decreases,\n",
    "i.e., as we seek to compute $\\loss'_\\llp$ for layers $\\ll$ closest to the input.\n",
    "\n",
    "This means \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss}{\\partial W_\\llp} & = & \\frac{\\partial \\loss}{\\partial y_\\llp} \\frac{\\partial y_\\llp}{\\partial W_\\llp} & = & \\loss'_\\llp \\frac{\\partial y_\\llp}{\\partial W_\\ip}\n",
    "\\end{array}\n",
    "$$\n",
    "will approach $0$.\n",
    "Since this term is used in the update to $W_\\ip$, we won't learn weights for the earliest layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now diagnose one reason that training of early Deep Learning networks was difficult\n",
    "- use of sigmoid activations were common (inspired by biology)\n",
    "- if activations were very large/small, we are in a region where the sigmoid's derivatives are $0$\n",
    "- even when non-zero,the maximum of the derivative of the sigmoid is much smaller than $1$\n",
    "- the end result was that deep networks suffered from Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The ReLU function's derivative does not suffer from this problem and ReLU's now tend to be\n",
    "the standard activation (barring other considerations, such as the range of outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Something seemingly as simple as taking derivatives turned out to have some important subtleties.\n",
    "\n",
    "The problem of gradients either shrinking to zero or growing too large is a real problem\n",
    "- It can still hinder the use of very deep (many layers) networks\n",
    "- This is particularly a problem in Recurrent networks\n",
    "    - The depth of the \"unrolled loop\" is the length of the input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will explore techniques to manage the issue of vanishing and exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.547px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

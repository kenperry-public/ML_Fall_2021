{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os \n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import unsupervised_helper\n",
    "%aimport unsupervised_helper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Derivatives, Gradients, Jacobians\n",
    "\n",
    "From basic calculus we are (hopefully) familiar with the *derivative*\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x}\n",
    "$$\n",
    "\n",
    "where\n",
    "$y = f(x)$ for some univariate functions $f$.\n",
    "\n",
    "But what about\n",
    "$$\n",
    "\\frac{\\partial \\y}{\\partial \\x}\n",
    "$$\n",
    "where $\\y = f(\\x)$ is a multivariate function (on vector $\\x$) with range that is *also* a vector.\n",
    "\n",
    "In general, $\\y$ and $\\x$ may be vectors and we need to define the *Jacobian* $\n",
    "\\frac{\\partial \\y}{\\partial \\x}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before giving the general form for the Jacobian, we illustrate it in steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scalar $y$, vector $\\x$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial \\x}\n",
    "$$\n",
    "- is the vector of length $ |\\x|$ of defined as\n",
    "$$\n",
    "\\left( \\frac{\\partial y}{\\partial \\x} \\right)_j = \\frac{\\partial y}{\\partial \\x_j}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "$| \\x | = 2$ and $y = \\x_1 * \\x_2$\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\\\\\n",
    "\\frac{\\partial y}{\\partial \\x} &  = & \n",
    "\\begin{pmatrix}\n",
    " \\frac{\\partial y}{\\partial \\x_1} & \\frac{\\partial y}{\\partial \\x_2}\n",
    "\\end{pmatrix}\\\\\n",
    "& = & \n",
    "\\begin{pmatrix}\n",
    " \\x_2 &  \\x_1\n",
    "\\end{pmatrix}\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be even more concrete: consider a Regression Task using the Mean Squared Error (MSE) loss function.\n",
    "\n",
    "$$\n",
    "\\loss_\\Theta = \\text{MSE}(\\y, \\hat{\\y}, \\Theta) = { 1\\over{m} } \\sum_{i=1}^m (  \\y^\\ip  - \\hat{\\y}^\\ip )^2\n",
    "$$\n",
    "\n",
    "Using $\\Theta$ to denote the vector of parameters\n",
    "- $\\Theta_0$ is the intercept\n",
    "- $\\Theta_j$ is the sensitivity of the loss to the independent variable (feature) $j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The derivative (gradient) of the scalar $\\loss_\\Theta$ with respect to vector $\\Theta$ is:\n",
    "$$\n",
    "\\nabla_\\Theta \\loss_\\Theta =\n",
    "\\begin{pmatrix}\n",
    " \\frac{\\partial}{\\partial \\Theta_0} \\text{MSE}(\\y, \\hat{\\y}, \\Theta) \\\\\n",
    " \\frac{\\partial}{\\partial \\Theta_1} \\text{MSE}(\\y, \\hat{\\y}, \\Theta) \\\\\n",
    " \\vdots \\\\\n",
    " \\frac{\\partial}{\\partial \\Theta_n} \\text{MSE}(\\y, \\hat{\\y}, \\Theta)\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are the details of the derivative of $\\loss_\\Theta$ with respect to independent variable $j$\n",
    "$$\n",
    "\\begin{array}{lll}\\\\\n",
    " \\frac{\\partial}{\\partial \\Theta_j} \\text{MSE}(\\y, \\hat{\\y}, \\Theta) & = &\n",
    "{ 1\\over{m} } \\sum_{i=1}^m  \\frac{\\partial}{\\partial \\Theta_j} (  \\y^\\ip  - \\hat{\\y}^\\ip )^2 & \\text{definition}\\\\\n",
    "& = & { 1\\over{m} } \\sum_{i=1}^m  {2 * (  \\y^\\ip  - \\hat{\\y}^\\ip ) \\frac{\\partial}{\\partial \\Theta_j}} \\hat{\\y}^\\ip & \\text{chain rule}\\\\\n",
    "& = & { 1\\over{m} } \\sum_{i=1}^m  {2 * (  \\y^\\ip  - \\hat{\\y}^\\ip ) \\frac{\\partial}{\\partial \\Theta_j}} (\\Theta * \\x^\\ip) & \\hat{\\y}^\\ip = \\Theta^T \\cdot \\x^\\ip\\\\\n",
    "& = & { 1\\over{m} } \\sum_{i=1}^m  {2 * (  \\y^\\ip  - \\hat{\\y}^\\ip ) \\x^\\ip_j}   \\\\\n",
    "& = & { 2 \\over{m} } \\sum_{i=1}^m  { (  \\y^\\ip  - \\hat{\\y}^\\ip ) \\x^\\ip_j}   \\\\\n",
    "\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector $\\y$, scalar $x$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\y}{\\partial x}\n",
    "$$\n",
    "- is a column vector with $|\\y|$ rows\n",
    "- defined as\n",
    "\n",
    "$$\n",
    "\\left( \\frac{\\partial \\y}{\\partial x} \\right)^\\ip = \\frac{\\partial \\y^\\ip}{\\partial x}\n",
    "$$\n",
    "\n",
    "Technically (and this will be important when we define higher dimensional gradients recursively)\n",
    "- is the vector of length $1$\n",
    "- whose *element* is a vector of length $|\\y|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "$ \\y = ( x^0, x^1, x^2 )$\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\\\\\n",
    "\\frac{\\partial \\y}{\\partial x} &  = & \n",
    "\\begin{pmatrix}\n",
    " \\frac{\\partial \\y^{(1)}}{\\partial x} \\\\\n",
    " \\frac{\\partial \\y^{(2)}}{\\partial x} \\\\\n",
    " \\frac{\\partial \\y^{(3)}}{\\partial x}\n",
    "\\end{pmatrix}\\\\\n",
    "& = & \n",
    "\\begin{pmatrix}\n",
    " 0 \\\\\n",
    " 1 \\\\\n",
    " 2\n",
    "\\end{pmatrix}\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector $\\y$, vector $\\x$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\y}{\\partial \\x}\n",
    "$$\n",
    "- is the vector of length $| \\x |$\n",
    "- whose *element* is a vector of length $|\\y]$\n",
    "- defined as\n",
    "\n",
    "$$\n",
    "\\left( \\frac{\\partial \\y}{\\partial \\x} \\right)^\\ip_j = \\frac{\\partial \\y^\\ip}{\\partial \\x_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "$ | \\x | = 2, y = ( \\x_1 + \\x_2, \\x_1 * \\x_2)$\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\\\\\n",
    "\\frac{\\partial \\y}{\\partial \\x} &  = & \n",
    "\\begin{pmatrix}\n",
    " \\frac{\\partial \\y^{(1)}}{\\partial \\x_1} & \\frac{\\partial \\y^{(1)}}{\\partial \\x_2}\\\\\n",
    " \\frac{\\partial \\y^{(2)}}{\\partial \\x_1} & \\frac{\\partial \\y^{(2)}}{\\partial \\x_2}\n",
    "\\end{pmatrix}\\\\\n",
    "& = & \n",
    "\\begin{pmatrix}\n",
    " 1 & 1 \\\\\n",
    " \\x_2 & \\x_1 \n",
    "\\end{pmatrix}\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensors and Generalized Jacobians\n",
    "\n",
    "A *tensor* is multi-dimensional collection of values.\n",
    "\n",
    "We are familiar with special cases\n",
    "- a vector is a tensor with $1$ dimension\n",
    "- a matrix is a tensor with $2$ dimensions\n",
    "\n",
    "A $D$-dimensional tensor is a collection of numbers with *shape*\n",
    "$$\n",
    "( n_1 \\times n_2 \\times \\ldots \\times n_D )\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can define the *Generalized Jacobian* \n",
    "$$\n",
    "\\frac{\\partial \\y}{\\partial \\x}\n",
    "$$\n",
    "\n",
    "analogous to how we defined the Jacobian.\n",
    "\n",
    "The main difference is that now  the indices $i$ and $j$ change from *scalars* to *tensors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let\n",
    "- the shape of $\\x$ be $(n_{x_1} \\times n_{x_2} \\times \\ldots n_{x_{D_x}})$\n",
    "- the shape of $\\y$ be $(n_{y_1} \\times n_{y_2} \\times \\ldots n_{y_{D_y}})$\n",
    "\n",
    "$$\n",
    "\\left( \\frac{\\partial \\y}{\\partial \\x} \\right)^\\ip_j\n",
    "$$\n",
    "- is \n",
    "the tensor with shape $\\left( (n_{y_1} \\times n_{y_2} \\times \\ldots n_{y_{D_y}}) \\times (n_{x_1} \\times n_{x_2} \\times \\ldots n_{x_{D_x}}) \\right)$\n",
    "- defined *recursively* as\n",
    "\n",
    "$$\n",
    "\\left( \\frac{\\partial \\y}{\\partial \\x} \\right)^\\ip_j = \\frac{\\partial \\y^\\ip}{\\partial \\x_j}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Note that \n",
    "- the number of dimensions of $\\y^\\ip$ is $|\\y| -1$\n",
    "- the number of dimensions of $\\x_j$ is $|\\x| -1$\n",
    "\n",
    "so the recursive call (RHS of equation) operates on an object of lesser dimension and hence will reduce to a base case (derivatives involving only vectors and scalars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where do these higher dimensional tensors come from ?\n",
    "\n",
    "They are omnipresent !\n",
    "- The mini batch index\n",
    "- multi-dimensional input data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mini batch index\n",
    "\n",
    "When TensorFlow shows you the shape of an object, it typically has one more dimension\n",
    "than \"natural\" and the leading dimension is `None`.\n",
    "\n",
    "That is because TensorFlow computes *on every element of the mini batch* simultaneously.\n",
    "\n",
    "So the leading index points to an input example.\n",
    "\n",
    "Hence the extra dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multidimensional data\n",
    "\n",
    "Lots of data is multi-dimensional.\n",
    "\n",
    "For examples images have a height, width and depth (number of color channels).\n",
    "\n",
    "Before we introduced Tensors, we \"flattened\" higher dimensional images into vectors.\n",
    "\n",
    "We then had to \"unflatten\" the scalar derivatives in order to rearrange them so as to correspond\n",
    "to the same index in the input from which they originated.\n",
    "\n",
    "For the most part, this flatten/unflatten paradigm is not necessary if we operate over Tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The derivatives that are needed for Gradient Descent often involve tensors.\n",
    "\n",
    "This module formalized what it means to take derivatives of higher dimensional objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

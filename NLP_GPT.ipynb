{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GPT: Generalized Pre-Training\n",
    "\n",
    "GPT is a sequence of increasingly powerful (and big) models of similar architecture.\n",
    "\n",
    "We introduced the family and the original model [here](NLP_Recent.ipynb#GPT:-Generalized-Pre-Training)\n",
    "\n",
    "The second and third generation models are 10 and 1000 times bigger (number of parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPT-2\n",
    "\n",
    "[paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "[Model card](https://github.com/openai/gpt-2/blob/master/model_card.md)\n",
    "\n",
    "[Summary](https://openai.com/blog/better-language-models/)\n",
    "\n",
    "- 48 Transformer blocks (4 times original)\n",
    "    - $n_\\text{heads} = 16 (?), d_\\text{head} = 96 (?)$\n",
    "        - $d_\\text{model} = n_\\text{heads} * d_\\text{head} = 1536$\n",
    "- 1.5 billion weights\n",
    "- Trained on\n",
    "    - Trained on 40GB of data, 10 times the amount of data as original GPT \n",
    "    - Sequence of 1024 tokens (2 times original)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Results: Zero shot\n",
    "- Tested on 8 tasks\n",
    "    - State of the art on 7 out of the 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPT-3\n",
    "[paper](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "[Model card](https://github.com/openai/gpt-3/blob/master/model-card.md)\n",
    "\n",
    "[Summary]()\n",
    "\n",
    "- 96 Transformer blocks(8 times original)\n",
    "    - $n_\\text{heads} = 96, d_\\text{head} = 128 (?)$\n",
    "    - $d_\\text{model} = n_\\text{heads} * d_\\text{head} = 12,288$\n",
    "- 1.5 billion weights\n",
    "- Trained on\n",
    "    - 570 GB of data (100 times GPT)\n",
    "    - [Common Crawl](https://commoncrawl.org/the-data/get-started/)\n",
    "        - web crawler over multiple years\n",
    "        - 570 GB (100 times GPT)\n",
    "        - 410 billion tokens\n",
    "    - Additional training sets, for experiments\n",
    "        - [Webtext2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n",
    "            - Web pages originating from highly ranked Reddit links\n",
    "            - 19 billion tokens\n",
    "        - Books\n",
    "            - 67 billion tokens\n",
    "        -Wikipedia\n",
    "            - 3 billion tokens\n",
    "    \n",
    "    - Sequence of 2048 tokens\n",
    "\n",
    "    - 190K KWh of electricity used in training\n",
    "        - \\\\$ 0.22 per KW hour $\\approx \\$ 42K$ electricity used to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can see from the following graph how the computation times increase by orders of magnitude over the generations of GPT\n",
    "- GPT-3 small $\\approx$ GPT\n",
    "- GPT-3 XL $\\approx$ GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Compute time</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/LM_GPT_compute.png\" width=80%></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Picture from: https://arxiv.org/pdf/2005.14165.pdf</center></td>\n",
    "    </tr>   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How much would it cost you to train GPT-3 ?\n",
    "- Amazon Cloud\n",
    "    - G5 instance\n",
    "        - NVidia A10G Tensor Core GPUs @ 250 Tflops/GPU\n",
    "        - 8 GPU instance (2 Pflops) @\\\\$10/hour (with yearly contract; \\\\$16\\hour on-demand)\n",
    "            - \\\\$240 per 2Pflops-day\n",
    "            \n",
    "        \n",
    "- GPT-3 $\\approx$ 3000 Pflop-days\n",
    "    - $ 3000/2 = 1500$ days G5 instances to get 3000 Pflops-days\n",
    "    - Cost = 1500 * \\\\$240/day = \\\\$360K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# WebText: a new training set\n",
    "\n",
    "One key to the success of GPT-2 (and later generations) was a newly created training set that was scraped from the Web.\n",
    "\n",
    "The most common web-scraped dataset is [Common Crawl](https://commoncrawl.org/)\n",
    "- large, diversified\n",
    "- quality problems ?\n",
    "    - Large set of pages pointed to are \"gibberish\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The GPT team tried to create a high-quality crawl by using a curated approach to links\n",
    "- Based on Reddit\n",
    "- Only follow links originating from highly-ranked (high \"karma\") Reddit pages\n",
    "\n",
    "The result is called WebText\n",
    "- 40GB; 8MM documents\n",
    "- removed any Wikipedia\n",
    "    - since it is included in many of the benchmark tasks whose performance we want to measure out of sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-task learning\n",
    "\n",
    "One area of recent interesting is *multi-task learning*\n",
    "- Training a model to implement multiple tasks\n",
    "\n",
    "A model that implements a single task computes\n",
    "$$\\pr{\\text{output | input}}$$\n",
    "\n",
    "A model that implements several tasks computes\n",
    "$$\\pr{\\text{output | input, task-id }} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When training a model for multiple tasks, the training examples would look something like:\n",
    "$$\\begin{array}[lll] \\\\\n",
    "(  \\mathsf{Translate \\; to \\;French} , & \\text{English text} ,  & & \\text{French Text}) \\\\\n",
    "( \\mathsf{Answer \\; the \\; question} , & \\text{document} , & \\text{question} , & \\text{answer}) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Text is almost a universal encoding so NLP is a natural way of expressing multiple tasks.\n",
    "\n",
    "So a natural extensions of a Language Model is to solve multiple tasks\n",
    "- Encode your specific task as an input that can be handled by a Language Model\n",
    "- That's one advantage of Byte Pair Encoding\n",
    "    - No special per-task pre-processing needed for a task's training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will take the idea of Multi-task learning one step further\n",
    "- Learning how to solve a task **without** explicitly training a model !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Learning to learn\n",
    "\n",
    "The GPT family explores some deep questions.\n",
    "\n",
    "We are familiar with teaching a NN a task via several approaches\n",
    "- Completely Supervised Training\n",
    "- Supervised Pre-Training with Fine-Tuning\n",
    "\n",
    "But can a NN learn to solve a task *without having seen training examples for the task* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This question can be framed as follows\n",
    "- Given a trained Language Model LM\n",
    "- Can LM model be *used* for a new target task T, with examples $(\\x^\\ip_T, \\y^\\ip_T)$\n",
    "    - By giving LM a set $E$ consisting of $k$ examples for task T\n",
    "    \n",
    "Notice the word *used* rather than *trained*\n",
    "- the weights of LM are **not** changed\n",
    "- the examples in $E$ are only used to \"prime\" LM to understand the new task T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are variations of the question dependent on the size $k$ of examples\n",
    "\n",
    "- **Few shot learning**: $10 \\le k \\le 100$ typically\n",
    "- **One shot learning**: $k = 1$\n",
    "- **Zero shot learning** $k=0$\n",
    "\n",
    "A picture will help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Few/One/Zero shot learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/LM_Few_Shot_Training.png\"\" width=80%></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Picture from: https://arxiv.org/pdf/2005.14165.pdf</center></td>\n",
    "    </tr>   \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Is this even possible ?!\n",
    "\n",
    "Let's look at the reported results from the third generation GPT-3 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Few/One/Zero shot learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/LM_Few_Shot_Accuracy.png\"\" width=80%></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Picture from: https://arxiv.org/pdf/2005.14165.pdf</center></td>\n",
    "    </tr>   \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How is that possible ? Some theories\n",
    "\n",
    "**Theory 1**\n",
    "\n",
    "- The training set contains explicit instances of these out of sample tasks\n",
    "\n",
    "**Theory 2**\n",
    "\n",
    "- The super-large training sets  contain *implicit* instances of these out of sample tasks\n",
    "    - For example: an English-language article quoting a French speaker in French with English translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One thing that jumps out from the graph:\n",
    "- Bigger models are more likely to exhibit meta-learning\n",
    "\n",
    "**Theory 3**\n",
    "\n",
    "The training sets are so big that the model \"learns\" to create groups of examples with a common theme\n",
    "- Even with the large number of parameters, the model capacity does not suffice for example memorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another thing to consider\n",
    "- The behavior of an RNN depends on *all* previous inputs\n",
    "    - It has memory (latent state, etc.)\n",
    "    \n",
    "So Few Shot Learning may work by \"priming\" the memory with parameters for a specific task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Social concerns\n",
    "\n",
    "The team behind GPT is very concerned about potential misuse of Language Models.\n",
    "\n",
    "To illustrate, they conducted an experiment in having a Language Model construct news articles\n",
    "- Select title/subtitle of a genuine news article\n",
    "- Have the Language Model complete the article from the title/subtitle\n",
    "- Show humans the genuine and generated articles and ask them to judge whether the article was written by a human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Human accuracy in detecting model generated news articles</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/LM_GPT_model_generated_news.png\" width=80%></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Picture from: https://arxiv.org/pdf/2005.14165.pdf</center></td>\n",
    "    </tr>   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The bars show the range of accuracy across the 80 human judges.\n",
    "\n",
    "- 86% accuracy detecting articles created by a really bad model (the control)\n",
    "- 50% accuracy detecting articles created by the biggest models\n",
    "\n",
    "It seems that humans might have difficulty distinguishing between genuine and generated articles.\n",
    "\n",
    "The fear is that Language Models can be used\n",
    "- to mislead\n",
    "- to create offensive speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

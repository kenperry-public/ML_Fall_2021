{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import neural_net_helper\n",
    "%aimport neural_net_helper\n",
    "\n",
    "nnh = neural_net_helper.NN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Impediments to learning\n",
    "\n",
    "The updating of weights, used by Gradient Descent to minimize the loss, can be inhibited in less-than-obvious manners.\n",
    "\n",
    "In this module, we explore these impediments.\n",
    "\n",
    "This will motivate the creation of a new class of Layer-types: Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Proper scaling of inputs\n",
    "\n",
    "We briefly explore the statistical properties of the outputs of a layer.\n",
    "- We show how some of these properties can inhibit learning (weight update)\n",
    "- Will motivate the Normalization Layer-type, which will maintain good properties of layer outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Importance of zero centered inputs (for each layer)\n",
    "[Efficient Backprop paper, LeCunn98](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "\n",
    "**Zero centered** means average (over the training set) value of each feature of examples is mean $0$.\n",
    "\n",
    "Gradient descent updates each element of a layer $\\ll$'s weights $\\W_\\llp$ by\n",
    "the per-example losses \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss^\\ip }{\\partial W_\\llp} & = & \\frac{\\partial \\loss^\\ip}{\\partial \\y_\\llp^\\ip} \\frac{\\partial \\y_\\llp^\\ip}{\\partial \\W_\\llp} \n",
    "\\end{array}\n",
    "$$\n",
    "summed over examples $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look into the per example loss in more detail.\n",
    "\n",
    "\n",
    "Since $\\W_\\llp$ is a vector, the derivative wrt $\\W_\\llp$ is a vector of derivatives:\n",
    "$$\n",
    "\\frac{ \\partial{\\y_\\llp^\\ip} } { \\partial \\W_\\llp } =\n",
    "\\begin{pmatrix} \n",
    "\\ldots , \\frac{ \\partial{\\y_\\llp^\\ip} } { \\partial \\W_{\\llp,} }, \\ldots, \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Examining the $j^{th}$ element of the derivative vector:\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\frac{ \\partial{\\y_\\llp^\\ip} } { \\partial \\W_{\\llp,j} } & = & \\frac{ \\partial{ a_\\llp ( \\y_{(\\ll-1)}\\cdot \\W_\\llp ) } } { \\partial \\W_{\\llp,j}} &  \\text{ when layer } \\ll \\text{ is Dense since }  y_\\llp = a_\\llp ( \\y_{(\\ll-1)} \\cdot \\W_\\llp ) \\; \\\\\n",
    "& = &  \\frac{ \\partial{ a_\\llp ( \\y_{(\\ll-1)}\\cdot \\W_\\llp )} } { \\partial (\\y_{(\\ll-1)}\\cdot \\W_{\\llp})}\n",
    "       \\frac{ \\partial{(\\y_{(\\ll-1)}\\cdot \\W_\\llp)} }{ \\W_{\\llp,j} } & \\text{Chain rule} \\\\\n",
    "& =& a'_\\llp \\y_{(\\ll-1),j}^\\ip & \\text{ since }  \\y_{(\\ll-1)}\\cdot \\W_\\llp = \\sum_j { ( \\y_{(\\ll-1),j} * \\W_{\\llp,j} ) }\\\\\n",
    " & & & \\text{where } a' = \\frac{ \\partial{ a_\\llp ( \\y_{(\\ll-1)}\\cdot \\W_{\\llp}) } } { \\partial (\\y_{(\\ll-1)}\\cdot \\W_{\\llp})}\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is for $\\loss^\\ip$, the per-example loss for example $i$.\n",
    "\n",
    "The (total) Loss $\\loss$ is averaged across all $m'$ examples in the mini-batch\n",
    "\n",
    "So the derivative of the Loss (with respect to the $j^{th}$ weight) $\\frac{\\partial \\loss }{\\partial W_{\\llp, j}}$ will have the term\n",
    "$$\n",
    "\\sum_{i=0}^{m'} {  \\y_{(\\ll-1),j}^\\ip  }\n",
    "$$\n",
    "\n",
    "Thus, the update to $\\W_{\\llp, j}$ will be proportional to the average (across the $m'$ examples) of the $j^{th}$ input to layer $\\ll$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be concrete, let's focus on layer $1$, where \n",
    "$$\\y_{(\\ll -1),j} = \\x_j$$\n",
    "so that\n",
    "$$\n",
    "\\sum_{i=0}^m {  \\y_{(\\ll-1),j}^\\ip  } = \\bar\\x_j\n",
    "$$\n",
    "i.e., the average (across examples) value of input feature $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the particular case that the average $\\bar{\\x}_j$ of *every* feature $j$ has the same sign:\n",
    "- updates in all weight dimensions will have the same sign (the sign of $a'$)\n",
    "- Example: two dimensions.  The weight space is $\\W_{\\llp,0} \\times \\W_{\\llp,1}$\n",
    "    - We can navigate the the loss surface by moving in the weight space north-east or south-west only ! \n",
    "    - this can result in an indirect \"zig-zag\" toward the optimum\n",
    "        - To get to a point south-east from the current, we have to zig-zag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Although we have illustrated this issue using layer $1$, the issue applies to each layer.\n",
    "\n",
    "In fact, the issue may be more likely in deeper layers\n",
    "- when the activation of layer $(\\ll-1)$ is *not* zero-centered, e.g., the ReLU and sigmoid\n",
    "\n",
    "This will motivate the creation of a new layer type whose purpose will be to keep the inputs to successive layers zero-centered.\n",
    "\n",
    "**Note**\n",
    "\n",
    "Although we zero center the $m$ examples in the training set, the $m' \\lt m$ examples in any mini-batch will not necessarily be zero mean in all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importance of unit variance inputs (weight initialization)\n",
    "\n",
    "The same argument we made for zero-centering a feature can be extended to it's variance:\n",
    "- the variance of feature $j$ over all training examples $i$ is the variance of $\\y_{(\\ll-1),j}$\n",
    "\n",
    "If the variance of features $j$ and $j'$ are different, their updates will happen at different rates.\n",
    "\n",
    "We will examine this in greater depth during our discussion of weight initialization.\n",
    "\n",
    "For now: it is desirable that the input to *each* layer have it's features somewhat normalized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Initialization\n",
    "\n",
    "Training is all about discovering good weights.\n",
    "\n",
    "As prosaic as it sounds: how do we *initialize* the weights before training ?\n",
    "Does it matter ?\n",
    "\n",
    "It turns out that the choice of initial weights does matter.\n",
    "\n",
    "Let's start with some *bad* choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bad choices\n",
    "\n",
    "### Too big/small\n",
    "\n",
    "Layers usually consist of linear operations (e.g., matrix multiplication and addition of bias)\n",
    "followed by a non-linear activation.\n",
    "\n",
    "The range of many activation functions includes large regions where the derivatives are near zero,\n",
    "usually corresponding to very large/small activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gradient Descent updates weights using the gradients.\n",
    "\n",
    "Obviously, if the gradients are all near-0, learning cannot occur.\n",
    "\n",
    "So one bad choice is any set of weights that tends to push activations to regions of the non-linear\n",
    "activation with zero gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identical weights\n",
    "\n",
    "Consider layer $\\ll$ with $n_\\ll$ units (neurons) implementing identical operations (e.g. FC + ReLu).\n",
    "\n",
    "Let  $\\W_{\\llp, k}$ denote the weights of unit $k$.\n",
    "\n",
    "Suppose we initialized the weights (and biases) of all units to the *same* vector.\n",
    "$$\n",
    "\\W_{\\llp, k} = \\w_\\llp, \\; 1 \\le k \\le n_\\ll\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider two neuron $j, j'$ in the same layer $\\ll$\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\y_{\\llp, j}  & = & a_\\llp ( \\w_\\llp \\y_{(\\ll-1)} + \\b_\\llp ) \\\\\n",
    "\\y_{\\llp, j'} & = & a_\\llp ( \\w_\\llp \\y_{(\\ll-1)} + \\b_\\llp ) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Both neuron will compute the same activation\n",
    "- Both neurons will have the same gradient\n",
    "- Both neurons will have the same weight update\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, the weights in layer $i$ will start off identical and will remain identical due to identical updates!\n",
    "\n",
    "Neurons/units $j$ and $j'$ will never be able to differentiate and come to recognize *different* features.\n",
    "\n",
    "This negates the advantage of multiple units in a layer.\n",
    "\n",
    "Many approaches use some for of random initialization to break the symmetry we just described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Glorot initialization\n",
    "\n",
    "We have previously argued that each element $j$ of the first input layer ($\\x_{(0),j}$) should\n",
    "have unit variance across the training set.  \n",
    "\n",
    "This was meant to ensure that the first layer's weights\n",
    "updated at the same rate and that the activations of the first layer fell into regions of the activation\n",
    "function that had non-zero gradients.\n",
    "\n",
    "But this is not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's assume for the moment that each element $j$ of the input vector $\\y_{(\\ll-1)}$ is mean $0$, unit variance\n",
    "and mutually independent.  \n",
    "\n",
    "So view each $\\y_{(\\ll-1),j}$ as an independent random variable with mean $0$\n",
    "and unit variance.  \n",
    "\n",
    "Furthermore, let's assume each element $\\W_{\\llp,j}$ is similarly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the dot product in layer $\\ll$ \n",
    "$$f_\\llp(\\y_{(\\ll-1)}, W_\\llp) = \\y_{(\\ll-1)} \\cdot W_\\llp$$\n",
    "\n",
    "Recall that layer $(\\ll-1)$ has $n_{(\\ll-1)}$ outputs.\n",
    "\n",
    "Thus, the dot product is the sum over $n_{(\\ll-1)}$ pair-wise products \n",
    "- $\\y_{(\\ll-1),j} * \\W_{\\llp,j}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *variance* of a product of random variables $X, Y$ \n",
    "[is](https://en.wikipedia.org/wiki/Variance#Product_of_independent_variables)\n",
    "\n",
    "$$\n",
    "\\text{Var}(X * Y) = \\mathbb{E}(X)^2 \\text{Var}(Y) + \\mathbb{E}(Y)^2 \\text{Var}(X) + \\text{Var}(X)\\text{Var}(Y)\n",
    "$$\n",
    "\n",
    "So \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\text{Var}(\\y_{(\\ll-1),j} * \\W_{\\llp,j}) & = & 0^2 * 1 + 0^2 * 1 + 1 * 1 \\\\\n",
    "& = & 1 & \\text{Since } \\y_{(\\ll-1),j} \\text{ and } \\W_{\\llp,j} \\text{are mean } 0 \\text{ variance } 1\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus \n",
    "- The variance of the dot product involving $n_{(\\ll-1)}$ pair-wise products\n",
    "- Is $n_{(\\ll-1)}$, not $1$ as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can force the dot product to have unit variance\n",
    "- By scaling each $\\W_{\\llp,j}$ by \n",
    "$$\n",
    "\\frac{1}{\\sqrt{n_{(\\ll-1)}}}\n",
    "$$\n",
    "\n",
    "This is the basis for *Glorot/Xavier Initialization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- Sets the initial weights to a number drawn from a\n",
    "mean $0$, unit variance distribution (either normal or uniform)\n",
    "- Multiplied by $\\frac{1}{\\sqrt{n_{(\\ll-1)}}}\n",
    "$.\n",
    "\n",
    "Note that we don't strictly need the requirement of *unit* variance \n",
    "- It suffices that the input and output variances are *equal*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This only partially solves the problem as it only ensures unit variance of the **input** to the activation function.\n",
    "\n",
    "The [original Glorot paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) justifies this\n",
    "- By assuming either a $\\tanh$ or sigmoid activation function\n",
    "- Which are approximately linear in the active region.\n",
    "- So the **output** of the activation function is equal to the input in this region\n",
    "- And is therefore unit variance as desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus far, we have achieved unit variance during the forward pass.\n",
    "\n",
    "During back propagation\n",
    "- It can be  shown that the scaling factor\n",
    "- Depends on the number of outputs $n_\\llp$ of layer $\\ll$, rather than the number of inputs $n_{(\\ll-1)}$\n",
    "- Thus, the scaling factor needs to be $\\frac{1}{\\sqrt{n_\\llp}}$ rather than $\\frac{1}{\\sqrt{n_{(\\ll-1)}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Taking the average of the two scaling factors gives a final factor of\n",
    "$\\frac{1}{\\sqrt{ \\frac{ n_{(\\ll-1)} + n_\\llp}{2} } } = \\sqrt{\\frac{2}{n_{(\\ll-1)} + n_\\llp}}\n",
    "$\n",
    "\n",
    "which is what you often see in papers using this form of initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kaiming/He initialization\n",
    "\n",
    "Glorot/Xavier initialization was tailored to two particular activation functions ($\\tanh$ or sigmoid).\n",
    "\n",
    "[Kaiming et al](https://arxiv.org/pdf/1502.01852.pdf) extended the results\n",
    "to the ReLU activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The ReLU activation has two distinct regions: one linear (for inputs greater than 0) and one all zero.\n",
    "\n",
    "The linear region of the activation corresponds to the assumption of the Glorot method.\n",
    "\n",
    "So if inputs to the ReLU are equally distributed around 0, this is approximately the same\n",
    "as the Glorot method with half the number of inputs.\n",
    "- that is: half of the ReLU's will be in the active region and half will be in the inactive region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Kaiming scaling factor is thus:\n",
    "$$\n",
    "\\sqrt{\\frac{2}{n_{(\\ll-1)}} }\n",
    "$$\n",
    "in order to preserve unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layer-wise pre-training\n",
    "\n",
    "In the early days of Deep Learning\n",
    "- Before good weight initialization techniques were discovered\n",
    "- A technique called *Layer-wise pre-training* was very popular\n",
    "\n",
    "We can motivate this technique by briefly introducing an Autoencoder network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/Autoencoder_vanilla.jpg\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An Autoencoder network has two parts\n",
    "- An Encoder, which takes input $\\x$ and \"encodes\" it into $\\z$\n",
    "- A Decoder, which takes the encoding $\\z$ and tries to reproduce $\\x$\n",
    "\n",
    "Each part has its own weights, which can be discovered through training, with examples\n",
    "- $\\langle \\X, \\y \\rangle = \\langle \\X, \\X \\rangle$\n",
    "\n",
    "That is: we are asking the output to be identical to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This will not be possible\n",
    "when the dimension of $\\z$ is less than the dimension of $\\x$.\n",
    "- $\\z$ is a *bottle-neck*\n",
    "\n",
    "$\\z$ becomes a *reduced-dimensionality* approximation of $\\x$.\n",
    "\n",
    "This is quite similar to discovering Principal Components.\n",
    "- We discover a small number of synthetic features $\\z$ that summarize the diversity of $\\y_{(\\ll-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What does this have to do with layer-wise initialization of weights ?\n",
    "\n",
    "Suppose we want to initialize the weights of layer $\\ll$\n",
    "- We *temporarily* create a two layer Autoencoder network with layer $\\ll$ serving the role of Encoder\n",
    "- We train this temporary Autoencoder\n",
    "- This initializes the weights of layer $\\ll$\n",
    "- We discard the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The weights we create\n",
    "- Are not random, they meet the Autoencoder task objective\n",
    "- Perhaps non-random weights are better initializers because they discover some structure of the input\n",
    "\n",
    "Transfer Learning (the subject of another module) works in a similar manner\n",
    "- Use the weights obtained from training on a Source task\n",
    "- To use as initial weights for a second Target task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normalization\n",
    "\n",
    "We addressed the importance of normalization of the inputs to layer $\\ll = 1$.\n",
    "\n",
    "The same argument applies to *all* layers $\\ll > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This motivates the introduction of a new class of layer-types: Normalization layers\n",
    "\n",
    "- These layer types attempt to keep the distribution of $\\y_{\\llp,j}$\n",
    "normalized through all layers $\\ll$.\n",
    "- They become necessary for *very deep* (large number of layers) networks\n",
    "\n",
    "Normalization layers were one of the innovations that advanced Deep Learning\n",
    "by enabling learning in networks of extreme depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batch normalization\n",
    "[Batch Normalization paper](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "The idea behind batch normalization:\n",
    "-  perform standardization  (mean $0$, standard deviation 1)\n",
    "at each layer, using the mean and standard deviation of each mini batch.\n",
    "\n",
    "- facilitates higher learning rate \n",
    "    - controlling the size of the derivative allows higher $\\alpha$ without increasing product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Experimental results show that the technique:\n",
    "- facilitates the use of much higher learning rates, thus speeding training.  Accuracy is not lost.\n",
    "- facilitates the use of saturating activations functions (e.g., $\\tanh$ and sigmoid) which otherwise are subject to vanishing/exploding gradients.\n",
    "- acts as a regularizer; reduces the need for Dropout\n",
    "    - L2 regularization (weight decay) has *no* regularizing effect when used with Batch Normalization !\n",
    "        - [see](https://arxiv.org/abs/1706.05350)\n",
    "        - L2 regularization affects scale of weights, and thereby learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Details\n",
    "\n",
    "Consider a FC layer $\\ll$ with $n_\\ll$ outputs and a mini-batch of size $m_B$.\n",
    "\n",
    "Each of the $n_\\llp$ outputs is the result of\n",
    "- passing a linear combination of $\\y_{(\\ll -1)}$ (*activation inputs*)\n",
    "-  through an activation $a_{\\llp,j}$ (*activation outputs*)\n",
    "\n",
    "We could choose to standardize either the activation inputs or the activation outputs.\n",
    "\n",
    "This algorithm standardizes the **activation inputs**.\n",
    "\n",
    "Standardization is performed relative to the mean and standard deviation of each batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Summary for layer $\\ll$ with equation $\\y_\\llp = a_\\llp( \\W_\\llp \\y_{(\\ll-1)})$\n",
    "- each output feature $j$: $\\y_{\\llp,j} = a_{\\llp,j}( \\W_{\\llp,j} \\y_{(\\ll-1)})$\n",
    "\n",
    "- Denote the dot product for output feature $j$ by $\\x_{\\llp,j} = \\W_{\\llp,} \\y_{(\\ll-1)}$\n",
    "- We will replace $\\x_{\\llp,j}$ by a \"standardized\" $\\z_{\\llp,j}$ to be described\n",
    "\n",
    "Rather than carrying along subscript $j$\n",
    "we write all operations on  the collection $\\x_{\\llp,j}$ as a vector operation on $\\x_\\llp$ for ease of notation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "$\n",
    "\\begin{split}\n",
    "1.\\quad & \\mathbf{\\mu}_B = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{\\mathbf{x}^\\ip} & \\quad  \\text{Batch mean}\\\\\n",
    "2.\\quad & {\\mathbf{\\sigma}_B}^2 = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{(\\mathbf{x}^\\ip - \\mathbf{\\mu}_B)^2} & \\quad \\text{Batch variance} \\\\\n",
    "3.\\quad & \\hat{\\mathbf{x}}^\\ip = \\dfrac{\\mathbf{x}^\\ip - \\mathbf{\\mu}_B}{\\sqrt{{\\mathbf{\\sigma}_B}^2 + \\epsilon}} & \\quad \\text{Standardize } \\x^\\ip\\\\\n",
    "4.\\quad & \\mathbf{z}^\\ip = \\gamma \\hat{\\mathbf{x}}^\\ip + \\beta  & \\quad \\text{De-Standardize } \\hat\\x^\\ip  \\text{ with learned mean and variance}\\\\\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So\n",
    "- $\\mathbf{\\mu}_B, \\mathbf{\\sigma}_B$ are vectors (of length $n_\\llp$) of \n",
    "    - the element-wise means and standard deviations (computed across the batch of $m_B$ examples)\n",
    "- $\\mathbf{\\hat{x}^{(i)}}$ is standardized $\\mathbf{x}^{(i)}$ \n",
    "\n",
    "**Note** the $\\epsilon$ in the denominator is there solely to prevent \"divide by 0\" errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is going on with $\\z^\\ip$ ?  \n",
    "\n",
    "Why are we constructing it with mean $\\beta$ and standard deviation $\\gamma$ ?\n",
    "\n",
    "$\\beta, \\gamma$ which are **learned** parameters.\n",
    "\n",
    "Why should $\\beta, \\gamma$ be learned ?\n",
    "\n",
    "At a minimum: it can't hurt:\n",
    "- it admits the possibility of the identity transformation\n",
    "    - which would be the simple standardization\n",
    "- but allows the unit to be non-linear when there is a benefit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, depending on the activation $a_{\\llp, j}$\n",
    "- $\\hat{\\x}_{\\llp,j}$ can wind up *within the active region* of the activation function\n",
    "\n",
    "This effectively makes our transformations linear, rather than non-linear, which are more powerful.\n",
    "\n",
    "By shifting the mean by $\\beta$ we gain the *option* to avoid this should it be beneficial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The final question is: what do we do at inference/test time, when all \"batches\" are of size 1 ?\n",
    "\n",
    "The answer is\n",
    "- compute a single $\\mathbf{\\mu}, \\mathbf{\\sigma}$ from the sequence of such values across all batches.\n",
    "- \"population\" statistics (over full training set\n",
    "- rather than \"sample\" statistics (from a single training batch).\n",
    "\n",
    "Typically a moving average is used.\n",
    "We refer readers to the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We create a new layer type $\\text{BN}$ to perform Batch Normalization to the inputs of any layer.\n",
    "\n",
    "Thus, it participates in both the forward (i.e., normalization) and backward (gradient computation)\n",
    "steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unbelievably good initialization\n",
    "\n",
    "We have seen several methods that attempt to create \"good\" weights\n",
    "Glorot and Kaiming weight initialization \n",
    "- ensures \"good\" distribution of outputs of a layer, given a good distribution of inputs to the layer\n",
    "\n",
    "Normalization (e.g., Batch Normalization)\n",
    "- tries to ensure good distribution of inputs across all layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are some initialization methods that attempt to create weights that are so good,\n",
    "that Normalization during training is no longer necessary.\n",
    "\n",
    "[Fixup initialization paper](https://arxiv.org/abs/1901.09321)\n",
    "- good initialization means you don't need normalization layers\n",
    "\n",
    "But good initialization can help too.\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Maintaining good properties of layer inputs throughout the depth of a multi-layer network\n",
    "is like priming a pump.\n",
    "\n",
    "Proper priming helps our learning to flow smoothly.\n",
    "\n",
    "We explored some of the stumbling blocks to learning (weight update) along with their solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.547px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
